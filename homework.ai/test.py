import asyncio
import os
import tempfile
import time
import threading

from pydub import AudioSegment
from pydub.utils import which
import simpleaudio as sa
import speech_recognition as sr
from edge_tts import Communicate

# èªéŸ³å¼•æ“è¨­å®š
os.environ["PATH"] += os.pathsep + r"C:\Program Files\FFMPEG\bin"
AudioSegment.converter = which("ffmpeg")
AudioSegment.ffprobe = which("ffprobe")

VOICE = "zh-TW-HsiaoChenNeural"
recognizer = sr.Recognizer()

# æ¨¡æ“¬å›æ‡‰èˆ‡æƒ…ç·’æª¢æ¸¬å‡½å¼ï¼ˆè«‹æ›¿æ›ç‚ºå¯¦éš›é‚è¼¯ï¼‰
def get_response(text, history): return "é€™æ˜¯å›æ‡‰ï¼š" + text
def detect_emotion(text, history, speaker="ai"): return "neutral"
def load_conversation(user_id): return []
def save_conversation(user_id, history): pass

# èªéŸ³æ’­æ”¾ï¼ˆé˜»å¡ç›´åˆ°æ’­æ”¾å®Œç•¢ï¼‰
async def speak_text_async(text: str):
    try:
        print(f"[ğŸ—£ï¸] èªéŸ³åˆæˆé–‹å§‹ï¼š{text}")
        communicate = Communicate(text, VOICE)
        mp3_path = os.path.join(tempfile.gettempdir(), f"mentora_tts_{int(time.time())}.mp3")
        wav_path = mp3_path.replace(".mp3", ".wav")

        await communicate.save(mp3_path)
        print("[ğŸ§] MP3 ä¸‹è¼‰å®Œæˆï¼Œè½‰æ›ç‚º WAV...")
        audio = AudioSegment.from_file(mp3_path, format="mp3")
        audio.export(wav_path, format="wav")

        print("[â–¶ï¸] æ’­æ”¾èªéŸ³ä¸­...")
        wave_obj = sa.WaveObject.from_wave_file(wav_path)
        play_obj = wave_obj.play()
        await asyncio.to_thread(play_obj.wait_done)

        os.remove(mp3_path)
        os.remove(wav_path)
        print("[âœ…] èªéŸ³æ’­æ”¾å®Œæˆ")
        await asyncio.sleep(0.5)  # æ’­æ”¾å®Œç¨ä½œç­‰å¾…ï¼Œé¿å…å¡éº¥å…‹é¢¨

    except Exception as e:
        print(f"[âŒ] æ’­æ”¾éŒ¯èª¤ï¼š{e}")

# èªéŸ³è½‰æ–‡å­—
async def speech_to_text_async(timeout=10):
    print("[ğŸ¤] ç­‰å¾…ä½¿ç”¨è€…èªªè©±...")
    with sr.Microphone() as source:
        recognizer.adjust_for_ambient_noise(source)
        try:
            audio = await asyncio.to_thread(recognizer.listen, source, timeout=timeout)
            text = await asyncio.to_thread(recognizer.recognize_google, audio, language="zh-TW")
            print(f"[ğŸ“] ä½¿ç”¨è€…èªªäº†ï¼š{text}")
            return text
        except sr.WaitTimeoutError:
            print("[âŒ›] æ²’æœ‰èªéŸ³è¼¸å…¥")
        except sr.UnknownValueError:
            print("[âš ï¸] èªéŸ³ç„¡æ³•è¾¨è­˜")
        except sr.RequestError as e:
            print(f"[âŒ] èªéŸ³è¾¨è­˜éŒ¯èª¤ï¼š{e}")
    return None

# ä¸»æµç¨‹
async def main_async():
    print("[ğŸš€] å•Ÿå‹• Mentora AI åŠ©ç†")
    user_id = input("è«‹è¼¸å…¥ä½¿ç”¨è€…åç¨±ï¼š").strip()
    if not user_id:
        print("ä½¿ç”¨è€…åç¨±ä¸å¯ç‚ºç©ºã€‚")
        return

    history = load_conversation(user_id)
    if history:
        last_input = history[-1]["user"]
        welcome = f"æ­¡è¿å›ä¾†ï¼Œ{user_id}ï¼æˆ‘è¨˜å¾—ä½ ä¸Šæ¬¡èªªï¼šã€Œ{last_input}ã€ï¼Œä»Šå¤©é‚„å¥½å—ï¼Ÿ"
    else:
        welcome = f"å¾ˆé«˜èˆˆç¬¬ä¸€æ¬¡è¦‹åˆ°ä½ ï¼Œ{user_id}ï¼Œè®“æˆ‘å€‘é–‹å§‹èŠå¤©å§ï¼"

    print(f"[Mentora] {welcome}")
    await speak_text_async(welcome)

    timeout = 5 * 60
    last_activity = time.time()

    while True:
        if time.time() - last_activity > timeout:
            print("[â³] è¶…é 5 åˆ†é˜æœªæ“ä½œï¼ŒçµæŸå°è©±ã€‚")
            break

        user_input = await speech_to_text_async()
        if not user_input:
            continue

        if user_input.lower() in ["exit", "quit", "é›¢é–‹"]:
            goodbye = "æ„Ÿè¬ä½ çš„åˆ†äº«ï¼ŒæœŸå¾…å†æ¬¡è¦‹åˆ°ä½  ğŸŒ¿"
            print(f"[Mentora] {goodbye}")
            await speak_text_async(goodbye)
            break

        try:
            response = get_response(user_input, history)
            print(f"[Mentora] {response}")
            await speak_text_async(response)

            emotion = detect_emotion(response, history, speaker="ai")
            history.append({
                "user": user_input,
                "ai": response,
                "emotion": emotion
            })
            save_conversation(user_id, history)
            last_activity = time.time()

        except Exception as e:
            print(f"[âŒ] ç³»çµ±éŒ¯èª¤ï¼š{e}")
            await speak_text_async("ç³»çµ±ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚")

if __name__ == "__main__":
    asyncio.run(main_async())
